{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.2\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\User\\Downloads\\68.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1.3.iii)\n",
    "#This had to be done before the other tasks to make sure \n",
    "# the preprocessing could funtion properly\n",
    "\n",
    "# Check for missing values and outliers\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Handling missing values (fill with empty strings)\n",
    "df['headline'] = df['headline'].fillna('')\n",
    "df['short_description'] = df['short_description'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1.3.i)\n",
    "# Preprocess the text\n",
    "\n",
    "# Set stop words and inititlze lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])  # Removing punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Removing stopwords and lemmatizing\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to 'headline' and 'short_description'\n",
    "df['headline'] = df['headline'].apply(preprocess)\n",
    "df['short_description'] = df['short_description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get common words\n",
    "def get_common_words(texts):\n",
    "    all_words = ' '.join(texts).split()\n",
    "    return Counter(all_words).most_common(10)\n",
    "\n",
    "# Apply Fuction \n",
    "science_headlines = df[df['category'] == 'SCIENCE']['headline']\n",
    "queer_headlines = df[df['category'] == 'QUEER VOICES']['headline']\n",
    "\n",
    "print(\"Most common words in SCIENCE headlines:\")\n",
    "print(get_common_words(science_headlines))\n",
    "\n",
    "print(\"Most common words in QUEER VOICES headlines:\")\n",
    "print(get_common_words(queer_headlines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3.ii)\n",
    "#  Analyze other features\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(word_tokenize(x)))\n",
    "sns.histplot(df, x='headline_length', hue='category', kde=True)\n",
    "plt.title('Headline Length Distribution by Category')\n",
    "plt.show()\n",
    "\n",
    "df['description_length'] = df['short_description'].apply(lambda x: len(word_tokenize(x)))\n",
    "sns.histplot(df, x='description_length', hue='category', kde=True)\n",
    "plt.title('Description Length Distribution by Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 1.3.iii) again to find incorrect data types\n",
    "# \n",
    "# # Check for blank values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for incorrect data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Outliers can be visualized using the histogram created earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the Dataset:\n",
    "\n",
    "The SCIENCE category had a more even spread of common words while in the QUEER VOICES category the word gay was by far the most common word, followed by several other less prevelent top ten words. This was true for both the headline and short_description category. The common words are very different between SCIENCE and QUEER VOICES SO I believe they should be easy enough to categorize correctly. There are no missing values in the important columns like headline or short description so that should be fine. The coloums have the right data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step had to be done as there were NaN cells in some columns\n",
    "# Drop rows with NaN values in specific columns\n",
    "df = df.dropna(subset=['headline', 'short_description'])\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task 2.4.i\n",
    "# Split data into train (70%), validation (15%), and test (15%) sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this split because it is common practice. The 70 percent train data is enough, the 15 percent validation is a good enough size to validate well and the 15 percent testing is enough data that the model hasnt seen to assess if it performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.4.ii\n",
    "# Save splits to csv files\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "valid_data.to_csv('valid.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)\n",
    "\n",
    "#index=False removes the unneccesary index making the dataset cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.5 \n",
    "\n",
    "# Preprocessing steps: lowercasing, removing punctuation, lemmatization\n",
    "def preprocess_data(df):\n",
    "    df['cleaned_headline'] = df['headline'].apply(preprocess)\n",
    "    df['cleaned_short_description'] = df['short_description'].apply(preprocess)\n",
    "    return df\n",
    "\n",
    "train_data = preprocess_data(train_data)\n",
    "valid_data = preprocess_data(valid_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "\n",
    "# Load train and validation sets (and the test set for later)\n",
    "# This also takes both headline and short descrition and converts \n",
    "# them to text so that both features can be used in the model.\n",
    "\n",
    "train_data['text'] = train_data['cleaned_headline'] + ' ' + train_data['cleaned_short_description']\n",
    "valid_data['text'] = valid_data['cleaned_headline'] + ' ' + valid_data['cleaned_short_description']\n",
    "test_data['text'] = test_data['cleaned_headline'] + ' ' + test_data['cleaned_short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to do some preprocessing to the 3 data sets to ensure it is the same acrossed all. I made everything lowercase, removed punctionaiton and reduced all words to the root form with lemitizaiton. I did this so that only the words and their meanings would be analyzed. That way the data is better for the models to categorize correctly. Additionally I dropped NaN data as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2.6 \n",
    "# Vectorize and train Logistic Regression model\n",
    "#Pipeline is a way to transform and fit the data to make sure that everything goes in sequence.\n",
    "#TfidfVectorizer converts the data into numbers using the (TF-IDF). This gives words mathmatical importance\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "#This trains the logistic regression model\n",
    "\n",
    "log_reg_pipeline.fit(train_data['text'], train_data['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This evaluates on the validation set. This will be done again with the other models below.\n",
    "val_preds = log_reg_pipeline.predict(valid_data['text'])\n",
    "print(f\"Logistic Regression Validation Accuracy: {accuracy_score(valid_data['category'], val_preds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2.6\n",
    "# Vectorize and train Random Forest model\n",
    "# Same process as mentioned above with pipeline and the vectorizer.\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Random forrest model training.\n",
    "rf_pipeline.fit(train_data['text'], train_data['category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This evaluates the random forrest on the validation set. This will be done again with the other models below.\n",
    "val_preds_rf = rf_pipeline.predict(valid_data['text'])\n",
    "print(f\"Random Forest Validation Accuracy: {accuracy_score(valid_data['category'], val_preds_rf)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use the two classifiers: Logistic regression and Random Forrest. I chose them because they are both models that lend themselves well to text data and categorical data. They are disticnt from eachother and I thought they would be good choices for this project. The random forrest is supposed to be a bit more accurate and complex as well so I wanted to see if that was true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 2.7\n",
    "# I chose to use the long short term memory deep learning model and train it for catergoization.\n",
    "\n",
    "# Preparing the data for the LSTM model\n",
    "train_data['text'] = train_data['cleaned_headline'] + ' ' + train_data['cleaned_short_description']\n",
    "valid_data['text'] = valid_data['cleaned_headline'] + ' ' + valid_data['cleaned_short_description']\n",
    "test_data['text'] = test_data['cleaned_headline'] + ' ' + test_data['cleaned_short_description']\n",
    "\n",
    "X_train = train_data['text'].astype(str)\n",
    "X_valid = valid_data['text'].astype(str)\n",
    "X_test = test_data['text'].astype(str)\n",
    "y_train = train_data['category']\n",
    "y_valid = valid_data['category']\n",
    "y_test = test_data['category']\n",
    "\n",
    "# Tokenizing the text data because LSTM models require it.\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "# Padding sequences to ensure equal length because LSTM models require it.\n",
    "# This makes sure that all the data that goes into the model has the same length.\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_valid_pad = pad_sequences(X_valid_seq, maxlen=100)\n",
    "\n",
    "# Encoding the labels to make categories have a numeric valuess\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_valid_enc = label_encoder.transform(y_valid)\n",
    "\n",
    "# Building the LSTM model\n",
    "model = Sequential()\n",
    "#this is the embedding layer\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "# This spatial dropout helps with overfittling \n",
    "model.add(SpatialDropout1D(0.2))\n",
    "#This helps the model with memory \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# This makes the classification binary, with probabilty between 0 and  1\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the LSTM model, I chose 4 epochs to start with a batch size of 32\n",
    "history = model.fit(X_train_pad, y_train_enc, epochs=4, batch_size=32, validation_data=(X_valid_pad, y_valid_enc))\n",
    "\n",
    "# Save the trained model for further steps\n",
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3.8 \n",
    "I chose Accuracy as my primary metric. I made this decision because the 50-50 split of the dataset. I think it will be the best and most simple way to represnt whether the model is accurate. I will also use the F1 score to check model performance in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 3.9\n",
    "# Evaluate Logistic Regression\n",
    "train_preds = log_reg_pipeline.predict(train_data['text'])\n",
    "print(f\"Logistic Regression Train Accuracy: {accuracy_score(train_data['category'], train_preds)}\")\n",
    "print(f\"Logistic Regression Validation Accuracy: {accuracy_score(valid_data['category'], val_preds)}\")\n",
    "\n",
    "# Evaluate Random Forest\n",
    "train_preds_rf = rf_pipeline.predict(train_data['text'])\n",
    "print(f\"Random Forest Train Accuracy: {accuracy_score(train_data['category'], train_preds_rf)}\")\n",
    "print(f\"Random Forest Validation Accuracy: {accuracy_score(valid_data['category'], val_preds_rf)}\")\n",
    "\n",
    "# Evaluate LSTM Modelon the validation set\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_valid_pad, y_valid_enc)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model performed well! \n",
    "logistic regression the lowest: 91 percent accuracy on validation\n",
    "Random forrest was in the middle with 94 percent accuracy on validation \n",
    "The LTSM model was 95 percent accurate with validation. \n",
    "The logistic regression model lost 4 percentage points between the training accuracy and the validation accuracy.\n",
    "The random forrest lost 6 percent between the training and validaiton accuracy. The training for random forrest was 100 percent accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 3.10\n",
    "# Predict on validation set\n",
    "val_preds_log_reg = log_reg_pipeline.predict(valid_data['text'])\n",
    "val_preds_rf = rf_pipeline.predict(valid_data['text'])\n",
    "y_pred_probs = model.predict(X_valid_pad)\n",
    "y_pred_lstm = (y_pred_probs > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = valid_data.copy()\n",
    "comparison_df['log_reg_preds'] = val_preds_log_reg\n",
    "comparison_df['rf_preds'] = val_preds_rf\n",
    "comparison_df['lstm_preds'] = y_pred_lstm\n",
    "comparison_df['actual'] = valid_data['category']\n",
    "\n",
    "# Find rows where predictions differ from the actual labels\n",
    "log_reg_errors = comparison_df[comparison_df['actual'] != comparison_df['log_reg_preds']]\n",
    "rf_errors = comparison_df[comparison_df['actual'] != comparison_df['rf_preds']]\n",
    "lstm_errors = comparison_df[comparison_df['actual'] != comparison_df['lstm_preds']]\n",
    "\n",
    "# Display error examples\n",
    "print(\"Logistic Regression Errors:\")\n",
    "print(log_reg_errors.head())\n",
    "print('-----')\n",
    "\n",
    "print(\"Random Forest Errors:\")\n",
    "print(rf_errors.head())\n",
    "print('-----')\n",
    "\n",
    "print(\"LSTM Errors:\")\n",
    "print(lstm_errors.head())\n",
    "\n",
    "# Additional insights into common errors\n",
    "common_errors = comparison_df[\n",
    "    (comparison_df['log_reg_preds'] != comparison_df['actual']) &\n",
    "    (comparison_df['rf_preds'] != comparison_df['actual']) &\n",
    "    (comparison_df['lstm_preds'] != comparison_df['actual'])\n",
    "]\n",
    "\n",
    "print(\"Common Errors Across All Models:\")\n",
    "print(common_errors.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3.10 The error analysis showed that the logistic regression and the random forrest struggled more than the LSTM model. The logistic regression and the random forrest classified some of the same Science headlines as queer voices. While the LSTM model also had some errors, they were less than the other two and different in the sense that they were more random. This analysis reinforces the idea that deep learning models are superior to tradtional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 3.11\n",
    "# Adjusting Logistic Regression (changing regularization parameter from 1 to 0.5)\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(C=0.5, random_state=42))\n",
    "])\n",
    "\n",
    "log_reg_pipeline.fit(train_data['text'], train_data['category'])\n",
    "val_preds = log_reg_pipeline.predict(valid_data['text'])\n",
    "print(f\"Adjusted Logistic Regression Validation Accuracy: {accuracy_score(valid_data['category'], val_preds)}\")\n",
    "\n",
    "# Adjusting Random Forest (increasing number of trees)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(train_data['text'], train_data['category'])\n",
    "val_preds_rf = rf_pipeline.predict(valid_data['text'])\n",
    "print(f\"Adjusted Random Forest Validation Accuracy: {accuracy_score(valid_data['category'], val_preds_rf)}\")\n",
    "\n",
    "# Adjusting LSTM Model (adding more layers and change the number of epochs)\n",
    "# Adding more LSTM layers and changing epochs from 4 to 10\n",
    "model_improved = Sequential()\n",
    "model_improved.add(Embedding(input_dim=5000, output_dim=128))\n",
    "model_improved.add(SpatialDropout1D(0.2))\n",
    "model_improved.add(LSTM(100, return_sequences=True))\n",
    "model_improved.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model_improved.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_improved.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the improved model\n",
    "history_improved = model_improved.fit(X_train_pad, y_train_enc, epochs=10, batch_size=32, validation_data=(X_valid_pad, y_valid_enc))\n",
    "\n",
    "# Evaluate the improved model on the validation set\n",
    "val_loss_improved, val_accuracy_improved = model_improved.evaluate(X_valid_pad, y_valid_enc)\n",
    "print(f\"Improved Validation Loss: {val_loss_improved}\")\n",
    "print(f\"Improved Validation Accuracy: {val_accuracy_improved}\")\n",
    "\n",
    "# Save the improved model for further steps\n",
    "model_improved.save('improved_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 3.11\n",
    "Changes:\n",
    "I changed the regularization from 1 to 0.5 for the logistic regression to see if that would improve it.\n",
    "\n",
    "I added more trees to the random forrest to hope to improve the accuracy from 100 to 200\n",
    "\n",
    "I added an addtitional layers to the ltsm as well as more epochs: from 4 to 10 hoping to improve accuracy.\n",
    "\n",
    "Results:\n",
    "the logistic regression model became worse, with 87 percent accuracy when compaired to the original 91 percent.\n",
    "\n",
    "the random forrest model stayed about the same it was originally 93.67 and moved to 93.73.\n",
    "\n",
    "the LSTM deep learning model came out about the same as well going from the original 95.27 to the updated 95.37\n",
    "\n",
    "The LSTM Came out the winner again with 95 percent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task 3.12\n",
    "\n",
    "\n",
    "# Load the improved model\n",
    "model = load_model('improved_lstm_model.h5')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_valid_pad, y_valid_enc, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# predictons on the validation set\n",
    "y_pred_probs = model.predict(X_valid_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")  # Convert probabilities to binary class predictions\n",
    "\n",
    "# Calculate and print metrics\n",
    "val_accuracy_manual = accuracy_score(y_valid_enc, y_pred)\n",
    "print(f\"Validation Accuracy (Manual): {val_accuracy_manual}\")\n",
    "\n",
    "# classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_valid_enc, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the cross validation were good! The validation loss was pretty low .28 and the validation accuracy was high, 95 percent. \n",
    "The f1 score for Science was really high at 97 but the f1 score for Queer voices was not as good but still high at 90 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load and preprocess the test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenizing the text data because LSTM models require it.\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding sequences to ensure equal length because LSTM models require it.\n",
    "# This makes sure that all the data that goes into the model has the same length.\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = load_model('improved_lstm_model.h5')\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_probs = model.predict(X_test_pad)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "# Encode the true labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['category'])  # Fit on training labels\n",
    "y_test = test_data['category']\n",
    "y_test_enc = label_encoder.transform(y_test)  # Transform true labels\n",
    "\n",
    "# If the model was trained with encoded labels\n",
    "y_test_pred_encoded = y_test_pred.flatten()  # Flatten the predictions if necessary\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_enc, y_test_pred_encoded, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for the test were significantly lower than the cross validation. The test accuracy was 70 percent, down 25 percent from 95. Additionally the f1 score for SCIENCE was abismal, only 19 percent. Im not sure why this model performed so poorly on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.14\n",
    "# Load and preprocess the test set\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenizing the text data\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True, oov_token='<UNK>')\n",
    "\n",
    "# Combine train and validation text for tokenizer\n",
    "combined_texts = pd.concat([X_train, X_valid], axis=0)\n",
    "tokenizer.fit_on_texts(combined_texts)\n",
    "\n",
    "# Prepare test data\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Encode the test labels using the same LabelEncoder fitted on training data\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['category'])  # Fit on training labels\n",
    "y_test = test_data['category']\n",
    "y_test_enc = label_encoder.transform(y_test)  # Transform true labels\n",
    "\n",
    "# Combine training and validation data and retrain the model\n",
    "X_combined = pd.concat([X_train, X_valid])\n",
    "y_combined = pd.concat([train_data['category'], valid_data['category']], axis=0)\n",
    "\n",
    "# Tokenize and pad the combined data\n",
    "X_combined_seq = tokenizer.texts_to_sequences(X_combined)\n",
    "X_combined_pad = pad_sequences(X_combined_seq, maxlen=100)\n",
    "y_combined_enc = label_encoder.transform(y_combined)\n",
    "\n",
    "# Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Retrain the model with the combined training and validation data\n",
    "model.fit(X_combined_pad, y_combined_enc, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Save the retrained model\n",
    "model.save('retrained_lstm_model.h5')\n",
    "\n",
    "# Evaluate the retrained model on the test set\n",
    "# Load the retrained model\n",
    "model = load_model('retrained_lstm_model.h5')\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_probs = model.predict(X_test_pad)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_enc, y_test_pred, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retraining with the train and validation sets made a huge impact on the performence of the model. It went back to being 95 percent accurate and dramatically improved the f1 score on SCIENCE. Apparently there are some major faults with the model but with this retraining the model is again very accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
